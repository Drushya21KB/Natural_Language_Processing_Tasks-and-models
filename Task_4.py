# -*- coding: utf-8 -*-
"""Day 5 workshop(NLP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tzPESGtilgFrHOnCNlYHNeB9tsu0I7E7

# **Tokenization**

1.   Word Tokenization: Splitting the text into words
2.   Sentence : Splitting text into sentences
"""

import nltk

nltk.download_shell()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt_tab')

text = '''Nature conservation has never been more needed. Specific ecosystems, such as wetlands, play a crucial role in mitigating the dual crises of biodiversity and climate change.
Yet, these lands, which are home to unique wildlife that cannot survive elsewhere, are being lost at an unprecedented rate.
Approximately 15% of global peatlands have been drained or degraded.
This represents a significant loss, considering peatlands occupy only about 3% of the Earth’s land surface but store more carbon than all the world’s forests combined.'''

words = word_tokenize(text)

sentances = sent_tokenize(text)

print(f'Words: {words}', end='')

print(f'Sentences: {sentances}', end='')

nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

stop_words

len(stop_words)

remove_stpwrd = [word for word in words if word.lower() not in stop_words]

len(words)

print(f'after removal of stopwords: {remove_stpwrd}')

len(remove_stpwrd)

"""**Stemming: Bringing the word into their root or base form(eg: Running - Run)**"""

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

stemm = [stemmer.stem(words) for words in remove_stpwrd]

print(f'Stemmed Words: {stemm}')

"""**Lemetization - bring back words into dictionary form**"""

from nltk.stem import WordNetLemmatizer

lemmat = WordNetLemmatizer()

nltk.download('wordnet')

lem = [lemmat.lemmatize(words) for words in remove_stpwrd]

print(f'Lemmatized Words: {lem}')

"""**Part Of Speech**"""

nltk.download('averaged_perceptron_tagger_eng')

pos_tag = nltk.pos_tag(remove_stpwrd)

print(f'Pos Tags:{pos_tag}')

print(f'Pos T')

+!pip install SpeechRecognition
!pip install transformers
!pip install torch

import speech_recognition as sr
from transformers import pipeline

#Initiating the Voice Recognizer
recognizer = sr.Recognizer()

#Initiating the HUgging Face Sentimental Analysis Pipleline
senti_analyzer = pipeline('sentiment-analysis')

def speech_input():
  with sr.Microphone() as source:
    print('Please speak something..')
    recognizer.adjust_for_ambient_noise(source)
    audio = recognizer.listen(source)
    print('Voice Recognized!!')
    try:
      print('Your Voice is Reconizing...wait!!')
      text = recognizer.recognize_google(audio)
      print(f'Text:{text}')
      return text
    except sr.UnknownValueError:
      print("Unable to recognize your voice.. Please try again")
      return None

#Function for Analysing Sentiment of the converted text
def analyze_sentiment(text):
  senti = senti_analyzer(text)
  senti_label = senti['label']
  senti_score = senti['score']
  print(f'Sentiment Label: {senti_label} with Score: {senti_score}')
  return senti_label, senti_score

# Function to make main loop to continuosly listen for voice inputs
def main():
  print('Starting the voice recognition...')
  while True:
    print('\n..Listening for voice input...')
    text = listen_for_speech()
    if text:
      senti, score = analyze_senti(text)
      print(f'Analyzed Sentiment: {senti} and Score: {score}')
    else:
      print('No Voice Detected, Please speak again..')

if __name__ == '__main__':
  main()

"""# **Categorizing the news**"""

!pip install gradio

import nltk
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

cat = pd.read_json("/content/News_Category_Dataset_v3.json", lines=True)

print(cat.head())

cat.isnull().sum()

cat = cat[['headline', 'short_description', 'category']]

cat['full_text'] = cat['headline'] + " " + cat['short_description']

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha()]   # keep only words (remove punctuations/numbers)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

nltk.download('punkt_tab')

cat['cleaned_headline'] = cat['headline'].apply(preprocess)

# Only keep 'headline', 'short_description', 'category'
cat = cat[['headline', 'short_description', 'category']]

# Fill missing values
cat['headline'] = cat['headline'].fillna('')
cat['short_description'] = cat['short_description'].fillna('')

# Combine 'headline' + 'short_description'
cat['full_text'] = cat['headline'] + " " + cat['short_description']

# Preprocessing
cat['cleaned_text'] = cat['full_text'].apply(preprocess)

X_train, X_test, y_train, y_test = train_test_split(cat['cleaned_text'], cat['category'], test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000)  # Limit number of features
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vect, y_train)

y_pred = model.predict(X_test_vect)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

plt.figure(figsize=(8,3))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=False, cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

def predict_news_category(headline, short_description):
    combined_text = headline + " " + short_description
    processed = preprocess(combined_text)
    vectorized = vectorizer.transform([processed])
    prediction = model.predict(vectorized)[0]
    return prediction

interface = gr.Interface(
    fn=predict_news_category,
    inputs=[
        gr.Textbox(lines=2, label="Headline"),
        gr.Textbox(lines=3, label="Short Description")
    ],
    outputs="text",
    title="News Headline + Description Category Classifier",
    description="Enter a news headline and its short description to predict the news category!"
)

interface.launch()

cat['headline'][40]

cat['short_description'][40]

cat['category'][40]
