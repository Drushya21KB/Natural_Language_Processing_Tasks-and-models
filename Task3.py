# -*- coding: utf-8 -*-
"""Day 3 workshop(NLP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usuFZwLvTqTXO9jYJbrSJnQhGjL9678A

**NLP - NLTK**
"""

import nltk

nltk.download_shell()

ind = nltk.corpus.indian

ind.fileids()

names = nltk.download('names')

nam = nltk.corpus.names

nam.fileids()

gb = nltk.download('gutenberg')

gb = nltk.corpus.gutenberg

gb.fileids()

hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')

hamlet

len(hamlet)

nltk.download('punkt_tab')

mac = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')

mac

hamlet[:20]

nltk.download('stopwords')

sw = nltk.corpus.stopwords.words('english')

sw

len(sw)

fw = nltk.FreqDist(hamlet)
fw

fw.most_common(10)

"""Tokenization - splitting into words"""

import pandas as pd

email = pd.read_csv('/content/emails.csv')

email.info()

email.head()

email.isnull().sum()

fe = email['text']
tar = email['spam']

from sklearn.model_selection import train_test_split

fetrain, fetest, tartrain, tartest = train_test_split(fe, tar, test_size=0.2, random_state=42)

"""to covert text to numerical - CountVectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

countv = CountVectorizer(stop_words='english')

fetrain_v = countv.fit_transform(fetrain)
fetest_v = countv.transform(fetest)

from sklearn.naive_bayes import MultinomialNB

email = MultinomialNB()
email.fit(fetrain_v, tartrain)

pred = email.predict(fetest_v)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(tartest, pred)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix, classification_report

cm = confusion_matrix(tartest, pred)

cm

from matplotlib import pyplot as plt

import seaborn as sns

plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**2nd Dataset**"""

ads = pd.read_csv('/content/Social_Network_Ads.csv')

ads.info()

ads.head()

ads.isnull().sum()

ads.drop('User ID', axis=1)

X = ads.drop('Purchased', axis=1)
y = ads['Purchased']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.naive_bayes import GaussianNB

classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf  = confusion_matrix(y_test, y_pred)
print(conf)

plt.figure(figsize=(4, 3))
sns.heatmap(conf, annot=True, cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""# **Sentimental analysis - naive bayes - multinominal type**"""

imdb = pd.read_csv('/content/IMDB Dataset.csv')

imdb.info()

imdb.head()

imdb.isnull().sum()

imdb['sentiment'] = imdb['sentiment'].map({'positive': 1, 'negative': 0})

fea = imdb['review']
targ = imdb['sentiment']

fea_train, fea_test, targ_train, targ_test = train_test_split(fea, targ, test_size=0.2, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(stop_words='english', max_features=5000)

featrain_v = tfidf.fit_transform(fea_train)
fea_test_v = tfidf.transform(fea_test)

IMDB = MultinomialNB()
IMDB

IMDB.fit(featrain_v, targ_train)

predimdb = IMDB.predict(fea_test_v)

predimdb

ac_imdb = accuracy_score(targ_test, predimdb)
ac_imdb

cm_imdb = confusion_matrix(targ_test, predimdb)
cm_imdb

plt.figure(figsize=(4, 3))
plt.imshow(cm_imdb, interpolation='nearest', cmap='Reds')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.colorbar()

"""**Use gradio**"""

!pip install gradio

import gradio as gr

def predict_sentiment(review):
  review_vector = tfidf.transform([review])
  prediction = IMDB.predict(review_vector)
  if prediction == 1:
    return 'Positive'
  else:
    return 'Negative'

iface = gr.Interface(
    fn=predict_sentiment,
    inputs=gr.Textbox(lines=5, placeholder="Write your movie review here..."),
    outputs=gr.Textbox(),
    title="Movie Review Sentiment Prediction",
    description="Enter a movie review and find out if it's Positive or Negative!"
)

iface.launch()

tweet = pd.read_csv('/content/Twitter_Data.csv')

tweet.info()

tweet.head()

tweet.isnull().sum()

tweet['category'].value_counts()

tweet['clean_text'][40]

tweet['clean_text'] = tweet['clean_text'].fillna('')  # Replace NaN values with empty strings
m = tweet['clean_text']
n = tweet['category']
# Split the data into training and testing sets for both features (m) and target (n)
m_train, m_test, n_train, n_test = train_test_split(m, n, test_size=0.1, random_state=62)

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
m_train_tfidf = tfidf_vectorizer.fit_transform(m_train)
m_test_tfidf = tfidf_vectorizer.transform(m_test)
model = MultinomialNB()
model.fit(m_train_tfidf, n_train)

n_pred = model.predict(m_test_tfidf)

accuracy = accuracy_score(n_test, n_pred)
print(f"Accuracy: {accuracy}")

cm = confusion_matrix(n_test, n_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

def predict_tweet_sentiment(tweet_text):
    tweet_tfidf = tfidf_vectorizer.transform([tweet_text])
    prediction = model.predict(tweet_tfidf)[0]
    return prediction

new_tweets = [
    "This is an amazing product!",
    "I'm so angry about this service.",
    "Neutral tweet."
]
for tweet in new_tweets:
    sentiment = predict_tweet_sentiment(tweet)
    print(f"Tweet: {tweet} \nPredicted Sentiment: {sentiment}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr

# Load the Twitter data
tweet = pd.read_csv('/content/Twitter_Data.csv')

# Preprocess
tweet = pd.read_csv('/content/Twitter_Data.csv')

# Preprocess
tweet['clean_text'] = tweet['clean_text'].fillna('')
m = tweet['clean_text']
n = tweet['category']

# Drop rows with NaN values in the 'category' column
tweet = tweet.dropna(subset=['category'])  # Added line to drop NaN values

# Update m and n after dropping NaN values
m = tweet['clean_text']
n = tweet['category']

# Train-test split
m_train, m_test, n_train, n_test = train_test_split(m, n, test_size=0.1, random_state=62)
# TF-IDF vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
m_train_tfidf = tfidf_vectorizer.fit_transform(m_train)
m_test_tfidf = tfidf_vectorizer.transform(m_test)

# Model training
model = MultinomialNB()
model.fit(m_train_tfidf, n_train)

# Evaluation
n_pred = model.predict(m_test_tfidf)
accuracy = accuracy_score(n_test, n_pred)
print(f"Accuracy: {accuracy}")

# Confusion matrix
cm = confusion_matrix(n_test, n_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Prediction function for Gradio
def predict_tweet_sentiment(tweet_text):
    tweet_tfidf = tfidf_vectorizer.transform([tweet_text])
    prediction = model.predict(tweet_tfidf)[0]
    return prediction

# Gradio interface
iface = gr.Interface(
    fn=predict_tweet_sentiment,
    inputs=gr.Textbox(lines=3, placeholder="Type your tweet here..."),
    outputs=gr.Textbox(),
    title="Twitter Sentiment Prediction",
    description="Enter a tweet to predict whether it is Positive, Negative, or Neutral!"
)

iface.launch()

tweet['clean_text'][577]

"""**Fraud Detection**"""

f_d = pd.read_csv('/content/fake_news_dataset.csv')

f_d.info()

f_d.shape

f_d.head()

f_d.isnull().sum()

f_d = f_d.drop(['title', 'author', 'id', 'state', 'date_published', 'category', 'word_count'], axis=1)

f_d.head()

f_d = f_d.dropna(subset=['text', 'label'])
X = f_d['text']
y = f_d['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

def predict_fake_news(news_text):
    news_tfidf = tfidf_vectorizer.transform([news_text])
    prediction = model.predict(news_tfidf)[0]
    return "Fake" if prediction == 1 else "Real"

iface = gr.Interface(
    fn=predict_fake_news,
    inputs=gr.Textbox(lines=5, placeholder="Enter news text here..."),
    outputs=gr.Textbox(),
    title="Fake News Detection",
    description="Enter news text to determine if it's fake or real."
)

iface.launch()

f_d['text'][500]

f_d['text'][3041]

pip install gradio

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr

f_d = pd.read_csv('/content/fake_news_dataset.csv')


f_d = f_d.drop(['title', 'author', 'id', 'state', 'date_published', 'category', 'word_count'], axis=1)


f_d = f_d.dropna(subset=['text', 'label'])


X = f_d['text']
y = f_d['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)


model = MultinomialNB()
model.fit(X_train_tfidf, y_train)


y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")


cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


def predict_fake_news(news_text):
    if not news_text.strip():
        return "Please enter some text to predict."
    news_tfidf = tfidf_vectorizer.transform([news_text])
    prediction = model.predict(news_tfidf)[0]
    return "**FAKE NEWS**" if prediction == 1 else "**REAL NEWS**"


iface = gr.Interface(
    fn=predict_fake_news,
    inputs=gr.Textbox(lines=8, placeholder="Enter news article text here..."),
    outputs="text",
    title="Fake News Detection üö®",
    description="Enter a news article to check whether it's Fake or Real. (Trained on a dataset of news articles.)",
    theme="default"
)

iface.launch()

print(y.value_counts())

import re

def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    return text

f_d['text'] = f_d['text'].apply(clean_text)

f_d['label'] = f_d['label'].map({'Fake': 1, 'Real': 0})

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix

f_d = pd.read_csv('/content/fake_news_dataset.csv')

f_d = f_d.drop(['title', 'author', 'id', 'state', 'date_published', 'category', 'word_count'], axis=1)

f_d = f_d.dropna(subset=['text', 'label'])

f_d['label'] = f_d['label'].map({'Fake': 1, 'Real': 0})

import re

def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    return text

f_d['text'] = f_d['text'].apply(clean_text)

min_count = min(f_d['label'].value_counts())

fake_df = f_d[f_d['label'] == 1].sample(min_count, random_state=42)
real_df = f_d[f_d['label'] == 0].sample(min_count, random_state=42)

f_d_balanced = pd.concat([fake_df, real_df])

X = f_d_balanced['text']
y = f_d_balanced['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

def predict_fake_news(news_text):
    if not news_text.strip():
        return "‚ö†Ô∏è Please enter some text to predict."

    cleaned_text = clean_text(news_text)
    news_tfidf = tfidf_vectorizer.transform([cleaned_text])

    prediction = model.predict(news_tfidf)[0]
    return "üö® FAKE NEWS" if prediction == 1 else "‚úÖ REAL NEWS"

import gradio as gr

iface = gr.Interface(
    fn=predict_fake_news,
    inputs=gr.Textbox(lines=8, placeholder="Enter a news article or news sentence here..."),
    outputs="text",
    title="üì∞ Fake News Detection App",
    description="Enter a news article text and find out whether it's Fake or Real! (Model: Multinomial Naive Bayes, TF-IDF Vectorizer)",
    theme="default"
)

iface.launch()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download nltk resources if not already
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')

# Load Dataset
imdb = pd.read_csv('/content/IMDB Dataset.csv')

# Map sentiments
imdb['sentiment'] = imdb['sentiment'].map({'positive': 1, 'negative': 0})

# Initialize Lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Define User-Defined Cleaning and Lemmatizing Function
def clean_lemmatize_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)      # Remove special characters
    text = text.lower()                          # Lowercase
    words = text.split()                         # Tokenize
    filtered = [word for word in words if word not in stop_words]  # Remove stopwords
    lemmatized = [lemmatizer.lemmatize(word) for word in filtered] # Lemmatize
    return ' '.join(lemmatized)

# Apply BEFORE creating fea and targ
imdb['review'] = imdb['review'].apply(clean_lemmatize_text)

# Now divide features and target
fea = imdb['review']
targ = imdb['sentiment']

# Split the data
fea_train, fea_test, targ_train, targ_test = train_test_split(fea, targ, test_size=0.2, random_state=42)

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000)
featrain_v = tfidf.fit_transform(fea_train)
fea_test_v = tfidf.transform(fea_test)

# Train Model
IMDB = MultinomialNB()
IMDB.fit(featrain_v, targ_train)

# Predict
predimdb = IMDB.predict(fea_test_v)

# Accuracy
ac_imdb = accuracy_score(targ_test, predimdb)
print(f"Accuracy: {ac_imdb:.4f}")

# Confusion Matrix
cm_imdb = confusion_matrix(targ_test, predimdb)

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
plt.imshow(cm_imdb, interpolation='nearest', cmap='Reds')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.colorbar()
plt.show()
